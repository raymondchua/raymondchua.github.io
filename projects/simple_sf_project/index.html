<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning Successor Features the Simple Way | Raymond Chua </title> <meta name="author" content="Raymond Chua"> <meta name="description" content="A simple and elegant approach to learning Successor Features for Continual Reinforcement Learning. This project was accepted as a poster at the main conference of NeurIPS 2024."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://raymondchua.github.io/projects/simple_sf_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Raymond</span> Chua </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/mentorship/">Mentorship </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching &amp; Talks </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning Successor Features the Simple Way</h1> <p class="post-description">A simple and elegant approach to learning Successor Features for Continual Reinforcement Learning. This project was accepted as a poster at the main conference of NeurIPS 2024.</p> </header> <article> <p>Paper: <a href="https://arxiv.org/abs/2410.22133" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2410.22133</a></p> <p>Code: <a href="https://github.com/raymondchua/simple_successor_features" rel="external nofollow noopener" target="_blank">https://github.com/raymondchua/simple_successor_features</a></p> <h1 id="1-introduction">1. Introduction</h1> <p>Deep Reinforcement Learning plays a crucial role in ensuring that intelligent systems can be relied upon to navigate complex and non-stationary environments. However, learning representations that are robust towards forgetting and interference remains a challenge.</p> <p>Successor Features (SFs) offers a promising solution. However, learning SFs directly from complex, high-dimensional inputs, such as pixels, can lead to representation collapse, where the representations fail to capture key features in the data.</p> <p>In this blogpost, we introduce our new approach — Simple Successor Features (Simple SFs) — a streamlined, efficient way to learn SFs directly from pixels without requiring pre-training or the use of complex auxiliary losses. Our method not only prevents representation collapse but also demonstrates superior performance in mitigating interference, and to a lesser extent, forgetting. Simple SFs achieve these results in continual reinforcement learning across 2D and 3D environments as well as complex continuous control tasks in Mujoco.</p> <h1 id="2-what-are-successor-features-exactly">2. What are Successor Features exactly?</h1> <p>In Reinforcement Learning, an agent’s goal is to learn an optimal behavior (commonly known as policy function \(\pi\)) that corresponds to maximizing cumulative rewards. This is done by learning a Q-value function that estimates the future rewards for each state-action pair[1]. However, one challenge with this approach is that it can be hard to generalize knowledge across tasks, especially when the environment dynamics or the reward structure changes.</p> <p>Just to give a short history, SFs are an extension of Successor Representations (SRs), which were initially developed to generalize across tasks by focusing on the environment transition dynamics [2]. SRs rely on tabular basis representation —a lookup table that stores each state individually which limits their scalability. SFs are a distributed, function-approximation variant of SRs, allowing replacing the tabular basis representation with a basis features vector, making them more suitable for high-dimensional inputs, such as pixels[3].</p> <p>Just like SRs, we can decompose the Q-value function into two distinct components:</p> <ol> <li>Successor Features \(( \psi )\): These capture the expected occupancy of each state, essentially providing a predictive map on where the agent might end</li> <li>Task encoding \((\boldsymbol{w})\): Combined with the basis features, this component helps predict the reward value of a given state.</li> </ol> <p>Mathematically, this means that for each state-action pair (s,a) can be defined as the linear combination of \(\psi(s,a)\) and \(\boldsymbol{w}\):</p> \[\begin{align}Q(s,a) = \psi(s,a)^{\intercal}\boldsymbol{w}\end{align}\] <h1 id="3-challenges-of-learning-successor-features-from-pixels">3. Challenges of learning Successor Features from Pixels</h1> <p>In SRs, the tabular basis representations are usually pre-defined, such as using information about the spatial location of the agent to design this representation, which can also be adapted for basis features in SFs. However, in the scenario that the basis features have to be learned from high-dimensional inputs, such as pixels, things start to become tricky.</p> <p>The core learning mechanism for the basis features \(\phi \in \mathbb{R}^{n}\) and SFs \(\psi \in \mathbb{R}^n\) is the <strong>SF-Temporal Difference (SF-TD) learning rule</strong>, which updates the successor features based on the agent’s transitions. The SF-TD loss is defined as follows:</p> \[\begin{align}L_{\phi, \psi} = \frac{1}{2} \left \| \phi(S_{t+1}) + \gamma {\psi}(S_{t+1}, a, \boldsymbol{w})) - \psi(S_{t},A_{t}, \boldsymbol{w}) \right \|^2\end{align}\] <p>where action \(a \sim \pi(S_{t+1})\) and \(\gamma \in [0,1]\) is the discount factor. We consider each transition to be \((S_t, A_t, S_{t+1}, R_{t+1})\), where \(S_t\) is the state at time-step \(t\), \(A_t\) is the action at time-step \(t\), \(S_{t+1}\) is the next state at time-step \(t+1\) and \(R_{t+1}\) is the reward at time-step \(t+1\).</p> <p>When learning both the basis features \(\phi\) and the SFs \(\psi\) concurrently, this optimization can lead to representation collapse—where the learned features lose their discriminative characteristics across different states. This collapse occurs because the loss function is minimized if both \(\phi(\cdot)\) and \(\psi(\cdot)\) converge to constants across all states \(S\). Specifically, this happens when \(\phi(\cdot) = c_1\) and \(\psi(\cdot) = c_2\) with \(c_1 = (1-\gamma)c_2\).</p> <p>For a more detailed proof, See section 3.4 in the paper.</p> <p>To address this issue of representation collapse, some previous approaches have introduced <em>pretraining</em> [4] or <em>auxiliary losses</em> like reconstruction [5] and orthogonality [6] constraints. While these methods can help maintain feature diversity, they often add significant computational complexity. In contrast, our approach—<strong>Simple Successor Features (Simple SFs)</strong> —achieves similar resilience against collapse without requiring these additional components, as we discuss next.</p> <h1 id="4--simple-sfs-a-new-approach">4. Simple SFs: A New Approach</h1> <p>A key insight in designing Simple SFs is that <strong>the basis features \(\phi\) should not collapse to a constant</strong>, as this would allow the loss in Eq. 2 to be minimized trivially, leading to representation collapse. To address this, rather than directly optimizing Eq. 2, we leverage the definition in Eq. 1 and instead optimize the following losses:</p> <ol> <li> <em>Reward prediction loss</em> guides the task encoding vector \(\boldsymbol{w}\) to capture reward-relevant information from the environment. Here, the basis features \(\phi\) are treats as a constant:</li> </ol> \[\begin{align}L_{\boldsymbol{w}} = \frac{1}{2}\left \| R_{t+1} - \overline{\phi}(S_{t+1})^\top \boldsymbol{w} \right \|^2\end{align}\] <ol> <li> <em>Q-SF-TD loss</em> allows the SFs \(\psi\) to be learned using a Q-learning like loss, treating the task encoding vector \(\boldsymbol{w}\) as a constant learning only the SFs \(\psi\):</li> </ol> \[\begin{align}L_{\psi} = \frac{1}{2}\left \| \hat{y} - \psi(S_t, A_t, \boldsymbol{w})^{\top}\boldsymbol{w} \right \|^2\end{align}\] <p>By optimizing these losses, we ensure that the task encoding \(\boldsymbol{w}\) effectively captures reward-relevant information, while the SFs \(\psi\) are learned in a stable way without collapse, allowing Simple SFs to learn effectively from high-dimensional inputs, such as pixels.</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/our_model_original.png" alt="Figure 1: Architecture for Simple SFs for discrete actions" width="50%" height="50%"> <figcaption style="text-align: center; margin-top: 10px;">Figure 1: Architecture for Simple SFs for discrete actions</figcaption> </figure> <h1 id="5-environments">5. Environments</h1> <p>We evaluated our approach using 2D Minigrid, 3D Four Rooms environment and continuous control tasks in Mujoco, all within a continual learning setting. Agents are presented two tasks sequentially and then are re-exposed the same set of tasks in the same sequence a second time. All the studies were conducted exclusively using pixel observations as the primary motivation of this work is to address representation collapse when learning from high-dimensional pixel inputs.</p> <p>The baseline models that we compare our approach to are Double Deep Q-network agent [7] or DDPG (for continuous actions) [8], and agents learning SFs with constraints on their basis features \(\phi\), such as reconstruction loss [5], orthogonal loss [6], and unlearnable random features[6]. We also compare with an agent that learns SFs using a pre-training regime which does not require rewards from the environment [4].</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/environments.png" alt="Environments" width="50%" height="50%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 2: Environments used in our study. <strong>(a-c):</strong> We examined both egocentric (partially observable) and allocentric (fully observable) pixel observations in 2D Minigrid environments. Tasks included changes in reward locations (Inverted-L Walls) and combined changes in rewards and transition dynamics (Center-Wall). <strong>(d-f):</strong> Egocentric observations in a 3D Four Rooms environment, where the reward alternates between +1 for green and -1 for yellow in the first task, and -1 for green and +1 for yellow in the second task. <strong>(e):</strong> A slippery variant of the 3D Four Rooms environment, where selected agent actions are occasionally replaced by random actions based on a predefined slip probability. <strong>(g-h):</strong> Mujoco continuous control tasks, where agents either run forward and then backward, run forward at an increased speed in the second task, or switch from Half-Cheetah to Walker while being rewarded for running forward in the second task.</figcaption> </figure> <h1 id="6-results-for-2d-minigrid-and-3d-miniworld">6. Results for 2D Minigrid and 3D Miniworld</h1> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/minigrid_miniworld_total_returns.png" alt="2D and 3D Mazes results" width="80%" height="80%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 3: Continual Reinforcement Learning Evaluation with pixel observations in 2D Minigrid and 3D Four Rooms environment. <em>Replay buffer resets at each task transitions to simulate drastic distribution shifts</em>: Agents face two sequential tasks (Task 1 &amp; Task 2), each repeated twice (Exposure 1 &amp; Exposure 2). <strong>(a-c):</strong> The total cumulative returns accumulated during training. Overall, our agent, Simple SF (orange), shows notable superiority and exhibited better transfer in later tasks over both DQN (blue) and agents with added constraints. Importantly, constraints like reconstruction and orthogonality on basis features can impede learning.</figcaption> </figure> <h1 id="7-results-for-mujoco-continuous-control-tasks">7. Results for Mujoco Continuous Control Tasks</h1> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/mujoco_results.png" alt="Mujoco results" width="90%" height="90%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 4: Continual Reinforcement Learning results using pixel observations in Mujoco environment across 5 random seeds. <em>Replay buffer resets at each task transitions to simulate drastic distribution shifts.</em> we started with the half-cheetah domain in Task 1 where agents were rewarded for running forward. We then introduced three different scenarios in Task 2: <strong>(a)</strong> agents were rewarded for running backwards, <strong>(b)</strong> running faster, and, in the most drastic change, <strong>(c)</strong> switching from the half-cheetah to the walker domain with a forward running task. To ensure comparability across these diverse scenarios, we normalized the returns, considering that each task has different maximum attainable returns per episode. We did not evaluate APS (Pre-train) here because it struggles in the Continual RL setting, even in simpler environments such as the 2D Minigrid and 3D Miniworld.</figcaption> </figure> <h1 id="8-how-effectively-can-successor-features-be-decoded-into-successor-representations">8. How effectively can Successor Features be decoded into Successor Representations?</h1> <p>Can our approach learn Successor Features that capture the environment’s transition dynamics in the same way as Successor Representations [2]? To investigate, we created a simple non-linear decoder (as shown in Figure (a) below) that takes the learned SFs as inputs and compares the predicted outcomes with analytically computed SRs. The SRs are calculated using:</p> \[\begin{align}\text{SR} = (I - \gamma T)^{-1}\end{align}\] <p>where \(T\) is the transition probability matrix derived from the same policy used the SFs. We used mean squared error (MSE) to measure the similarity between the SFs and the predicted SRs in both egocentric and allocentric observations, using the Center-Wall environment after training the non-linear decoder. Results show that the SFs learned using our approach consistently achieve lower MSE compared to baseline models (Figure (b and c) below).</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/decoder_model_and_results.png" alt="SR Decoder" width="80%" height="80%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 5: <strong>(a)</strong> Architecture of the non-linear decoder model. <strong>(b-c)</strong> Comparison of the Mean Squared Error (MSE) between the learned SFs and the predicted SRs in the Center-Wall environment. Our approach consistently achieves lower MSE compared to baseline models, indicating that our SFs capture the environment's transition dynamics effectively.</figcaption> </figure> <h1 id="9-how-well-do-similar-successor-features-that-are-proximate-in-neural-space-correspond-to-proximity-in-physical-space">9. How well do similar Successor Features that are proximate in neural space correspond to proximity in physical space?</h1> <p>Using UMAP, we visualise the Successor Features in 2D space for the Center-Wall environment in both egocentric (partially observable) and allocentric (fully-observable) scenarios, as well as the 3D Four Rooms environment with egocentric observations. A geospatial color mapping is applied to the SFs to examine whether SFs that are close in physical space exhibit similar representations in neural space.</p> <p>The visualisation below shows that our approach consistently produces well-organized clusters in all the scenarios, unlike other baseline models. Notably, while some approaches using reconstruction or orthogonality constraints may yield well-clustered SFs, these clusters do not always translate into effective policy learning.</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/sf_vis.png" alt="UMAP results" width="90%" height="90%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 6: UMAP visualisation of Successor Features in 2D space for the Center-Wall environment in both egocentric (partially observable) and allocentric (fully-observable) scenarios, as well as the 3D Four Rooms environment with egocentric observations. A geospatial color mapping is applied to the SFs to examine whether SFs that are close in physical space exhibit similar representations in neural space. </figcaption> </figure> <h1 id="10-how-important-is-the-stop-gradient-operator-for-effective-learning">10. How important is the stop-gradient operator for effective learning?</h1> <p>In a sparse rewards environment, such as the 2D Minigrid and 3D Four Rooms environments, learning the basis features \(\phi\) and the task encoding vector \(\boldsymbol{w}\) concurrently using the Reward prediction loss (Eq. 3) can be challenging. A possible issue is that the basis features \(\phi \rightarrow \vec{0}\) , minimizing the loss but resulting in ineffective learning. Additionally, we want the task encoding vector \(\boldsymbol{w}\) to capture information solely about the rewards, without being influenced by the basis features \(\phi.\) To address this, we apply a stop-gradient operator on the basis features during learning with the Reward prediction loss (Eq. 3).</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/stop_gradient_analysis.png" alt="Stop-gradient operator" width="60%" height="60%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 7: Analysis of the use of stop-gradient operator in the Reward prediction loss. <strong>(a):</strong> We compare the performance of our approach with and without the stop-gradient operator in the 3D Four Rooms environment. The stop-gradient operator significantly improves the performance of our approach, indicating that it is crucial for effective learning.<strong>(b):</strong> Without the stop-gradient operator, the learned SFs fail to capture meaningful environment statistics after training. </figcaption> </figure> <h1 id="11-how-robust-are-the-successor-features-to-stochasticity-within-the-environment">11. How robust are the Successor Features to stochasticity within the environment?</h1> <p>To evaluate the robustness of SFs, we introduced stochasticity into the environment by applying a predetermined “slippery” probability, where agents’ selected actions are occasionally replaced with alternative random actions. This study was conducted across varying degrees of stochasticity (0.15, 0.3, and 0.45). Results show that our approach consistently demonstrates better learning efficiency compared to other baseline methods.</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/slippery_four_rooms_results.png" alt="Stochasticity analysis" width="90%" height="90%"> <figcaption style="text-align: left; margin-top: 10px;">Figure 8: Analysis of the robustness of SFs to stochasticity within the environment, using a predetermined <em>slippery</em> probability, resulting in actions being occasionally replaced with alternative random actions.</figcaption> </figure> <h1 id="12-how-efficient-is-our-approach-relative-to-other-methods">12. How efficient is our approach relative to other methods?</h1> <p>To study the efficiency of our approach, we examined various metrics, such as time taken for the agent to learn a good policy, the frames per second during backpropagation, and the overall time required for the agent to complete sequential tasks. As expected, approaches that use additional objectives, such as reconstruction or orthogonality constraints on the basis features \(\phi\), require more computational resources.</p> <figure style="text-align: center;"> <img src="/../assets/img/project_simple_sf/efficiency_analysis.png" alt="Efficiency analysis" width="80%" height="80%"> <figcaption style="text-align: center; margin-top: 10px;">Figure 9: Analysis of the efficiency of our approach relative to other methods. </figcaption> </figure> <h1 id="13-conclusion">13. Conclusion</h1> <p>In this blogpost, we introduced Simple Successor Features (Simple SFs), a streamlined approach to learning SFs directly from pixel observations without relying on complex auxiliary objectives such as reconstruction or orthogonality constraints. By focusing on an efficient and straightforward design, Simple SFs successfully mitigate issues like representation collapse and interference, achieving robust learning performance in dynamic, high-dimensional environments. The simplicity of our approach not only reduces computational demands but also enhances scalability, making it a practical choice for continual learning tasks. These results highlight the potential of Simple SFs as a powerful yet efficient solution in environments where both adaptability and computational efficiency are critical.</p> <h1 id="14-references">14. References</h1> <p>[1] Sutton, Richard S. “Reinforcement learning: An introduction.” <em>A Bradford Book</em> (2018).</p> <p>[2] Dayan, Peter. “Improving generalization for temporal difference learning: The successor representation.”<em>Neural computation</em>5.4 (1993): 613-624.</p> <p>[3] Borsa, Diana, et al. “Universal successor features approximators.”<em>arXiv preprint arXiv:1812.07626</em> (2018).</p> <p>[4] Liu, Hao, and Pieter Abbeel. “Aps: Active pretraining with successor features.”<em>International Conference on Machine Learning</em>. PMLR, 2021.</p> <p>[5] Machado, Marlos C., Marc G. Bellemare, and Michael Bowling. “Count-based exploration with the successor representation.”<em>Proceedings of the AAAI Conference on Artificial Intelligence</em>. Vol. 34. No. 04. 2020.</p> <p>[6] Touati, Ahmed, Jérémy Rapin, and Yann Ollivier. “Does zero-shot reinforcement learning exist?.”<em>arXiv preprint arXiv:2209.14935</em> (2022).</p> <p>[7] Van Hasselt, Hado, Arthur Guez, and David Silver. “Deep reinforcement learning with double q-learning.”<em>Proceedings of the AAAI conference on artificial intelligence</em>. Vol. 30. No. 1. 2016.</p> <p>[8] Lillicrap, T. P. “Continuous control with deep reinforcement learning.”arXiv preprint arXiv:1509.02971(2015).</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Raymond Chua. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"Discover the projects I love, spanning work, community, sports, and fun.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-mentorship",title:"Mentorship",description:"Below is a list of the brilliant students I&#39;ve had the privilege of mentoring through internships that not only focus on real-world applications but also encompass research components where no previous approaches exist. My role has involved advising both the interns and, occasionally, the companies on potential machine learning models, forming pipelines for data collection, model building, and performance evaluation.",section:"Navigation",handler:()=>{window.location.href="/mentorship/"}},{id:"nav-teaching-amp-talks",title:"Teaching &amp; Talks",description:"My journey of sharing passion and knowledge through teaching and public engagement.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-presented-our-recent-work-on-continual-rl-using-successor-features-computational-and-systems-neuroscience-cosyne-in-lisbon-portugal-with-a-selective-51-6-acceptance-rate",title:"Presented our recent work on Continual RL using Successor Features @ Computational and...",description:"",section:"News"},{id:"news-my-first-research-paper-titled-learning-successor-features-the-simple-way-on-which-i-am-the-primary-first-author-co-authored-with-arna-ghosh-christos-kaplanis-blake-richards-and-doina-precup-has-been-accepted-to-the-neural-information-processing-systems-conference-neurips-a-top-tier-machine-learning-conference-with-a-very-selective-25-8-acceptance-rate-check-out-the-blogpost-and-the-paper",title:"My first research paper titled \u201cLearning Successor Features the Simple Way,\u201d on which...",description:"",section:"News"},{id:"news-presented-our-recent-work-on-continual-rl-using-successor-features-and-memory-mechanisms-at-naisys-a-neuroai-conference-at-cold-spring-harbour-laboratory-at-ny-usa-stay-tuned-for-the-preprint-early-2025",title:"Presented our recent work on Continual RL using Successor Features and Memory Mechanisms...",description:"",section:"News"},{id:"news-gave-a-short-talk-on-my-work-on-continual-reinforcement-learning-at-the-foresight-institute-vision-weekend-event-in-san-francisco",title:"Gave a short talk on my work on Continual Reinforcement Learning at the...",description:"",section:"News"},{id:"news-invited-by-prof-paul-masset-to-give-a-talk-on-my-research-on-successor-features-and-continual-reinforcement-learning-at-his-lab-in-mcgill-university-montr\xe9al-the-talk-was-well-received-and-i-had-the-opportunity-to-discuss-my-work-with-several-members-of-his-lab-look-forward-to-future-collaborations-with-the-lab-and-hope-to-visit-again-soon",title:"Invited by Prof. Paul Masset to give a talk on my research on...",description:"",section:"News"},{id:"projects-neural-ai-reading-group-at-mila",title:"Neural-AI Reading Group at Mila",description:"I created the Neural-AI Reading Group at Mila in Spring 2019 to foster a community of researchers interested in the intersection of neuroscience and artificial intelligence. I am still involved in the organization of the group till this day.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-biological-and-artificial-rl-workshop-neurips-2019-amp-2020",title:"Biological and Artificial RL Workshop @ NeurIPS 2019 &amp; 2020",description:"Together with Feryal Behbahani, Sara Zannone, and others, I co-organized the Biological and Artificial Reinforcement Learning Workshop at NeurIPS 2019 and 2020.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-my-first-self-organized-road-cycling-trip",title:"My First Self-Organized Road Cycling Trip",description:"Explore why Mallorca is a cycling paradise as I share my adventures across its legendary routes and vibrant cyclist community. \ud83c\uddea\ud83c\uddf8",section:"Projects",handler:()=>{window.location.href="/projects/mallorca_cycling_project/"}},{id:"projects-learning-successor-features-the-simple-way",title:"Learning Successor Features the Simple Way",description:"A simple and elegant approach to learning Successor Features for Continual Reinforcement Learning. This project was accepted as a poster at the main conference of NeurIPS 2024.",section:"Projects",handler:()=>{window.location.href="/projects/simple_sf_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%61%79_%64%6F%74_%72_%64%6F%74_%63%68%75%61_%61%74_%67%6D%61%69%6C_%64%6F%74_%63%6F%6D (%70%6C%65%61%73%65 %72%65%70%6C%61%63%65 \u2018%64%6F%74\u2019 %77%69%74%68 '.' %61%6E%64 \u2018%61%74\u2019 %77%69%74%68 '@') %77%69%74%68 %6E%6F %73%70%61%63%65%73.","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-5746-3001","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=8pVlWRsAAAAJ&hl","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rachua","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/RaymondRChua","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/raymondrchua","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/raymondrchua.bsky.social","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>