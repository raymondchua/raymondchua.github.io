---
---

@article{chua2024successor,
  abbr={NeurIPS},
  title     = {Learning Successor Features the Simple Way},
  author    = {Raymond Chua and Arna Ghosh and Christos Kaplanis and Blake A. Richards and Doina Precup},
  journal =   {Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2024},
  note      = {Raymond Chua is the corresponding author. Blake A. Richards and Doina Precup are co-senior authors.},
  selected  = {true},
}

@article{chua2024cosyne,
  abbr={COSYNE},
  title     = {Learning Successor Features the Simple Way},
  author    = {Raymond Chua and Arna Ghosh and Christos Kaplanis and Blake A. Richards and Doina Precup},
  journal = {Computational and Systems Neuroscience (COSYNE) Abstracts},
  year      = {2024},
  address   = {Lisbon, Portugal},
  note      = {Peer-reviewed abstract. Corresponding author: Raymond Chua.},
  abstract  = {Biological agents like primates exhibit lifelong learning and adaptation, processes that are mirrored in the
  continual Reinforcement Learning (RL) frameworks of computational models. In RL, Successor Representations (SRs) enable
  RL agents to exhibit flexible behaviour by capturing the world's transition dynamics using discounted state visitations,
  akin to how animals learn from experiences over time. Successor Features (SFs) build upon SRs by using artificial neural
  networks to extract features, an area where SRs, with their discrete representations, fall short, especially in complex
  environments. However, learning SFs from scratch risks representation collapse and poor performance due to missed key
  features. To address this, two strategies have been explored: pre-training, unsuitable for continual RL scenarios due
  to out-of-distribution challenges, and auxiliary losses, including constraints like reconstruction on basis features.
  While the latter may reduce mismatches, it introduces inductive biases that hamper learning, as shown by our experiments.
  To tackle this, we devised a novel method that streamlines learning SFs from pixels, using a simplified modification of
  Temporal-difference (TD) loss, thus removing the necessity for pre-training and auxiliary losses. In single-task scenarios
  within both 2D (Minigrid) and 3D (Miniworld) mazes, our model matches with the standard RL model: Deep Q Network (DQN).
  More notably, in a continual RL setup involving two tasks, our model exhibits superior transferability when re-encountering
  tasks, outperforming both DQN and other SF models trained with auxiliary constraints. Additionally, through dimensionality
  reduction and geospatial color mapping, we visually observed that our SFs effectively capture the statistical structure
  of the environments. Interestingly, our findings also hint at similarities between our computational model and hippocampal
  predictive representations in dynamic contexts. This suggests new avenues for exploring diverse neural functionalities,
  in particular, how reward-driven representations differ from others, thereby enriching our understanding of neural
  adaptability and learning.},
}

@article{ROSCOW2021808,
    abbr={Review},
    title = {Learning offline: memory replay in biological and artificial reinforcement learning},
    journal = {Trends in Neurosciences},
    volume = {44},
    number = {10},
    pages = {808-821},
    year = {2021},
    issn = {0166-2236},
    doi = {https://doi.org/10.1016/j.tins.2021.07.007},
    url = {https://www.sciencedirect.com/science/article/pii/S0166223621001442},
    author = {Emma L. Roscow and Raymond Chua and Rui Ponte Costa and Matt W. Jones and Nathan Lepora},
    keywords = {computation, deep neural networks, hippocampus, Q-learning, reward},
    abstract = {Learning to act in an environment to maximise rewards is among the brainâ€™s key functions. This process has often been conceptualised within the framework of reinforcement learning, which has also gained prominence in machine learning and artificial intelligence (AI) as a way to optimise decision making. A common aspect of both biological and machine reinforcement learning is the reactivation of previously experienced episodes, referred to as replay. Replay is important for memory consolidation in biological neural networks and is key to stabilising learning in deep neural networks. Here, we review recent developments concerning the functional roles of replay in the fields of neuroscience and AI. Complementary progress suggests how replay might support learning processes, including generalisation and continual learning, affording opportunities to transfer knowledge across the two fields to advance the understanding of biological and artificial learning and memory.}
}

@article{chua2024cosyne,
  abbr={COSYNE},
  title     = {Learning Successor Features the Simple Way},
  author    = {Raymond Chua, Arna Ghosh, Christos Kaplanis, Blake A. Richards and Doina Precup},
  journal = {Computational and Systems Neuroscience (COSYNE) Abstracts},
  year      = {2024},
  address   = {Lisbon, Portugal},
  note      = {Peer-reviewed abstract. Corresponding author: Raymond Chua.},
  abstract  = {Biological agents like primates exhibit lifelong learning and adaptation, processes that are mirrored in the
  continual Reinforcement Learning (RL) frameworks of computational models. In RL, Successor Representations (SRs) enable
  RL agents to exhibit flexible behaviour by capturing the world's transition dynamics using discounted state visitations,
  akin to how animals learn from experiences over time. Successor Features (SFs) build upon SRs by using artificial neural
  networks to extract features, an area where SRs, with their discrete representations, fall short, especially in complex
  environments. However, learning SFs from scratch risks representation collapse and poor performance due to missed key
  features. To address this, two strategies have been explored: pre-training, unsuitable for continual RL scenarios due
  to out-of-distribution challenges, and auxiliary losses, including constraints like reconstruction on basis features.
  While the latter may reduce mismatches, it introduces inductive biases that hamper learning, as shown by our experiments.
  To tackle this, we devised a novel method that streamlines learning SFs from pixels, using a simplified modification of
  Temporal-difference (TD) loss, thus removing the necessity for pre-training and auxiliary losses. In single-task scenarios
  within both 2D (Minigrid) and 3D (Miniworld) mazes, our model matches with the standard RL model: Deep Q Network (DQN).
  More notably, in a continual RL setup involving two tasks, our model exhibits superior transferability when re-encountering
  tasks, outperforming both DQN and other SF models trained with auxiliary constraints. Additionally, through dimensionality
  reduction and geospatial color mapping, we visually observed that our SFs effectively capture the statistical structure
  of the environments. Interestingly, our findings also hint at similarities between our computational model and hippocampal
  predictive representations in dynamic contexts. This suggests new avenues for exploring diverse neural functionalities,
  in particular, how reward-driven representations differ from others, thereby enriching our understanding of neural
  adaptability and learning.},
}

@article{chua2022cosyne,
  abbr={COSYNE},
  title     = {Continual Reinforcement Learning with Multi-Timescale Successor Features},
  author    = {Raymond Chua and Christos Kaplanis and Blake A. Richards and Doina Precup},
  journal = {Computational and Systems Neuroscience (COSYNE) Abstracts},
  year      = {2024},
  address   = {Lisbon, Portugal},
  note      = {Peer-reviewed abstract. Corresponding author: Raymond Chua.},
  abstract  = {Learning and memory consolidation in the brain occur over multiple timescales. Inspired by this observation,
  it has been shown that catastrophic forgetting in reinforcement learning (RL) agents can be mitigated by consolidating Q-value
  function parameters at multiple timescales. In this work, we combine this approach with successor features, and show that by
  consolidating successor features and preferences \textit{learned over multiple timescales} we can further mitigate catastrophic
  forgetting. In particular, we show that agents trained with this approach rapidly recall previously rewarding sites in large environments,
  whereas those trained without this decomposition and consolidation mechanism do not.  These results therefore contribute to our
  understanding of the functional role of synaptic plasticity and memory systems operating at multiple timescales, and demonstrate
  that RL can be improved by capturing features of biological memory with greater fidelity.},
}




